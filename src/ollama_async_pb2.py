# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# NO CHECKED-IN PROTOBUF GENCODE
# source: ollama_async.proto
# Protobuf Python Version: 5.29.0
"""Generated protocol buffer code."""
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import runtime_version as _runtime_version
from google.protobuf import symbol_database as _symbol_database
from google.protobuf.internal import builder as _builder

_runtime_version.ValidateProtobufRuntimeVersion(
    _runtime_version.Domain.PUBLIC,
    5,
    29,
    0,
    '',
    'ollama_async.proto'
)
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()




DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x12ollama_async.proto\x12\x06ollama\"\x9f\x01\n\x0cModelRequest\x12\x12\n\nmodel_name\x18\x01 \x01(\t\x12\x0e\n\x06prompt\x18\x02 \x01(\t\x12\x38\n\nparameters\x18\x03 \x03(\x0b\x32$.ollama.ModelRequest.ParametersEntry\x1a\x31\n\x0fParametersEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\"\x9d\x01\n\rModelResponse\x12\x0e\n\x06output\x18\x01 \x01(\t\x12\x17\n\x0f\x63ompletion_time\x18\x02 \x01(\x02\x12\x33\n\x07metrics\x18\x03 \x03(\x0b\x32\".ollama.ModelResponse.MetricsEntry\x1a.\n\x0cMetricsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\x02:\x02\x38\x01\"8\n\x0eSessionRequest\x12\x12\n\nsession_id\x18\x01 \x01(\t\x12\x12\n\nmodel_name\x18\x02 \x01(\t\"9\n\x0fSessionResponse\x12\x0f\n\x07success\x18\x01 \x01(\x08\x12\x15\n\rerror_message\x18\x02 \x01(\t\"2\n\x0b\x43hatRequest\x12\x12\n\nsession_id\x18\x01 \x01(\t\x12\x0f\n\x07message\x18\x02 \x01(\t2\x89\x02\n\rOllamaService\x12\x39\n\x08RunModel\x12\x14.ollama.ModelRequest\x1a\x15.ollama.ModelResponse\"\x00\x12\x42\n\rCreateSession\x12\x16.ollama.SessionRequest\x1a\x17.ollama.SessionResponse\"\x00\x12;\n\x0b\x43hatMessage\x12\x13.ollama.ChatRequest\x1a\x15.ollama.ModelResponse\"\x00\x12<\n\nStreamChat\x12\x13.ollama.ChatRequest\x1a\x15.ollama.ModelResponse\"\x00\x30\x01\x62\x06proto3')

_globals = globals()
_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'ollama_async_pb2', _globals)
if not _descriptor._USE_C_DESCRIPTORS:
  DESCRIPTOR._loaded_options = None
  _globals['_MODELREQUEST_PARAMETERSENTRY']._loaded_options = None
  _globals['_MODELREQUEST_PARAMETERSENTRY']._serialized_options = b'8\001'
  _globals['_MODELRESPONSE_METRICSENTRY']._loaded_options = None
  _globals['_MODELRESPONSE_METRICSENTRY']._serialized_options = b'8\001'
  _globals['_MODELREQUEST']._serialized_start=31
  _globals['_MODELREQUEST']._serialized_end=190
  _globals['_MODELREQUEST_PARAMETERSENTRY']._serialized_start=141
  _globals['_MODELREQUEST_PARAMETERSENTRY']._serialized_end=190
  _globals['_MODELRESPONSE']._serialized_start=193
  _globals['_MODELRESPONSE']._serialized_end=350
  _globals['_MODELRESPONSE_METRICSENTRY']._serialized_start=304
  _globals['_MODELRESPONSE_METRICSENTRY']._serialized_end=350
  _globals['_SESSIONREQUEST']._serialized_start=352
  _globals['_SESSIONREQUEST']._serialized_end=408
  _globals['_SESSIONRESPONSE']._serialized_start=410
  _globals['_SESSIONRESPONSE']._serialized_end=467
  _globals['_CHATREQUEST']._serialized_start=469
  _globals['_CHATREQUEST']._serialized_end=519
  _globals['_OLLAMASERVICE']._serialized_start=522
  _globals['_OLLAMASERVICE']._serialized_end=787
# @@protoc_insertion_point(module_scope)
