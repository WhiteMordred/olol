# Roadmap d'impl√©mentation de la persistance avec TinyDB

## Introduction

Ce document pr√©sente la roadmap d√©taill√©e pour l'impl√©mentation d'un syst√®me de persistance de donn√©es bas√© sur TinyDB dans le projet OLOL. Il d√©crit les √©tapes, les d√©pendances entre les composants et les objectifs √† atteindre √† chaque phase du d√©veloppement.

**Objectifs principaux :**
- Remplacer les structures en m√©moire (verrous, dictionnaires) par une persistance bas√©e sur TinyDB
- Collecter des informations d√©taill√©es sur les ressources mat√©rielles des n≈ìuds
- Mettre en place un m√©canisme robuste de synchronisation des donn√©es entre TinyDB et la RAM
- Assurer une abstraction permettant une migration future vers d'autres syst√®mes de stockage
- Impl√©menter un syst√®me de registre des mod√®les pour la gestion centralis√©e
- D√©velopper un m√©canisme de file d'attente pour optimiser les inf√©rences et g√©rer la charge

## Phase 1 : Mise en place de l'infrastructure de persistance

### 1.1 Classe d'abstraction DatabaseManager (‚úì Compl√©t√©)

**Fichier :** `/src/osync/proxy/db/database.py`

La classe d'abstraction [`DatabaseManager`](/src/osync/proxy/db/database.py) a √©t√© impl√©ment√©e avec les fonctionnalit√©s suivantes :
- Interface CRUD compl√®te (Create, Read, Update, Delete)
- Singleton pour acc√®s global √† l'instance
- Gestion des chemins de stockage automatis√©e
- Horodatage automatique des documents
- Support de requ√™tes flexibles

**Relations :**
- Utilis√© par tous les autres composants n√©cessitant une persistance
- Point d'acc√®s unique via la fonction `get_db()`

### 1.2 Ajout de la d√©pendance √† TinyDB (‚úì Compl√©t√©)

**Fichier :** `/pyproject.toml`

La d√©pendance √† TinyDB a √©t√© ajout√©e au fichier pyproject.toml :
```toml
dependencies = [
    # ...autres d√©pendances existantes...
    "tinydb>=4.8.0",
]
```

### 1.3 Sch√©ma des donn√©es

D√©finition du sch√©ma des tables pour la persistance des donn√©es :

#### Table `servers`
```json
{
  "address": "host:port",
  "healthy": true,
  "load": 0.45,
  "last_check": "2025-04-30T14:30:00",
  "first_seen": "2025-04-28T10:15:20",
  "hardware_info": {
    "cpu": { ... },
    "ram": { ... },
    "gpu": { ... },
    "storage": { ... }
  }
}
```

#### Table `models`
```json
{
  "name": "llama3",
  "servers": ["host1:port", "host2:port"],
  "size_gb": 8,
  "parameter_count": "8B",
  "quantization": "Q4_K_M",
  "first_seen": "2025-04-15T08:20:00",
  "last_used": "2025-04-30T14:30:00",
  "usage_count": 157
}
```

#### Table `server_stats`
```json
{
  "server": "host:port",
  "timestamp": "2025-04-30T14:30:00",
  "type": "health|load|latency",
  "value": 0.75
}
```

#### Table `request_stats`
```json
{
  "timestamp": "2025-04-30T14:30:00",
  "period": "hourly|daily|weekly",
  "total_requests": 1458,
  "generate_requests": 857,
  "chat_requests": 523,
  "embedding_requests": 78,
  "average_latency_ms": 235.5
}
```

#### Table `config`
```json
{
  "key": "proxy_settings|load_balancing|retry_policy",
  "value": {},
  "updated_at": "2025-04-30T14:30:00"
}
```

#### Table `inference_queue`
```json
{
  "id": "request-uuid",
  "model": "llama3",
  "prompt": "Texte du prompt",
  "status": "pending|processing|completed|failed",
  "priority": 1,
  "created_at": "2025-04-30T14:30:00",
  "started_at": "2025-04-30T14:30:05",
  "completed_at": "2025-04-30T14:30:10",
  "server_assigned": "host:port",
  "batch_id": "batch-uuid"
}
```

## Phase 2 : Synchronisation DB-RAM et communication gRPC

### 2.1 Architecture de synchronisation

**Principes fondamentaux :**
- La base de donn√©es TinyDB est la source de v√©rit√© persistante
- Les structures en m√©moire sont utilis√©es pour les acc√®s rapides
- Toute modification est d'abord √©crite en DB puis synchronis√©e en RAM
- Un m√©canisme de chargement initial charge les donn√©es depuis TinyDB au d√©marrage

**M√©canisme de synchronisation :**
```mermaid
sequenceDiagram
    participant D as TinyDB
    participant R as RAM Cache
    participant C as Composant
    
    Note over D,R: D√©marrage du syst√®me
    D->>R: Chargement initial
    
    Note over D,R: Op√©ration d'√©criture
    C->>D: write_to_db()
    D-->>C: Confirmation
    D->>R: sync_to_ram()
    
    Note over D,R: Op√©ration de lecture
    C->>R: read_from_ram()
    R-->>C: Donn√©es
```

### 2.2 Impl√©mentation de la synchronisation

**Fichier :** `/src/osync/proxy/db/sync_manager.py`

**T√¢ches :**
- [ ] Cr√©er une classe `SyncManager` pour g√©rer la synchronisation
- [ ] Impl√©menter les m√©thodes de synchronisation bidirectionnelle
- [ ] Ajouter des m√©canismes de verrouillage pour les acc√®s concurrents
- [ ] G√©rer les cas de conflits et de r√©conciliation

**M√©thodes √† impl√©menter :**
```python
class SyncManager:
    def __init__(self):
        self.db = get_db()
        self._ram_cache = {}
        self._locks = {}
    
    def load_initial_state(self):
        """Charge l'√©tat initial depuis TinyDB vers la RAM."""
        
    def write_and_sync(self, table, data, doc_id=None):
        """√âcrit dans TinyDB et synchronise avec la RAM."""
        
    def read_from_ram(self, table, query=None):
        """Lit les donn√©es depuis la RAM."""
        
    def force_sync(self, table=None):
        """Force la synchronisation entre TinyDB et la RAM."""
```

## Phase 3 : Int√©gration gRPC et syst√®me de registre de mod√®les

### 3.1 Am√©lioration de la communication gRPC

**Fichier :** `/src/osync/service.py`

**T√¢ches :**
- [ ] √âtendre le service gRPC pour collecter toutes les m√©triques des n≈ìuds
- [ ] Ajouter des endpoints pour acc√©der √† toutes les fonctionnalit√©s d'Ollama
- [ ] Impl√©menter des m√©thodes pour la gestion des mod√®les √† distance

**Nouvelles m√©thodes √† ajouter :**
```python
def GetCompleteNodeStatus(self, request, context):
    """
    Collecte l'√©tat complet du n≈ìud, incluant les m√©triques syst√®me.
    """

def RemoteModelCommand(self, request, context):
    """
    Ex√©cute une commande sur un mod√®le √† distance (pull, push, delete).
    """
```

### 3.2 Syst√®me de registre des mod√®les

**Fichier :** `/src/osync/proxy/cluster/registry.py`

**T√¢ches :**
- [ ] Cr√©er un syst√®me centralis√© de gestion des mod√®les
- [ ] Impl√©menter les op√©rations CRUD pour les mod√®les
- [ ] G√©rer la synchronisation des mod√®les entre les n≈ìuds

**Fonctionnalit√©s cl√©s :**
- Pull des mod√®les sur des n≈ìuds sp√©cifiques
- Distribution intelligente des mod√®les bas√©e sur les ressources
- Suivi de l'utilisation et de la disponibilit√© des mod√®les
- V√©rification p√©riodique de la coh√©rence du registre

**M√©thodes √† impl√©menter :**
```python
class ModelRegistry:
    def __init__(self, cluster_manager):
        self.cluster_manager = cluster_manager
        self.sync_manager = SyncManager()
        
    def pull_model(self, model_name, target_nodes=None):
        """Pull un mod√®le sur les n≈ìuds sp√©cifi√©s ou auto-s√©lectionn√©s."""
        
    def remove_model(self, model_name, target_nodes=None):
        """Supprime un mod√®le des n≈ìuds sp√©cifi√©s."""
        
    def synchronize_models(self):
        """Synchronise la disponibilit√© des mod√®les entre tous les n≈ìuds."""
        
    def get_model_status(self, model_name=None):
        """R√©cup√®re le statut du/des mod√®le(s) dans le cluster."""
        
    def optimize_model_distribution(self):
        """Optimise la distribution des mod√®les selon la charge et les ressources."""
```

## Phase 4 : Syst√®me de file d'attente pour les inf√©rences

### 4.1 Architecture de la file d'attente

**Principes de conception :**
- File d'attente persistante stock√©e dans TinyDB
- Synchronisation en m√©moire pour les acc√®s rapides
- Nettoyage automatique des requ√™tes anciennes ou trait√©es
- Regroupement (batching) des demandes similaires
- Priorisation des requ√™tes

**Sch√©ma de la file d'attente :**
```mermaid
graph TD
    A[Client] -->|Requ√™te| B[File d'attente]
    B -->|Stockage| C[TinyDB]
    B -->|Synchronisation| D[Cache RAM]
    E[Scheduler] -->|Planification| D
    E -->|Allocation| F[Serveurs d'inf√©rence]
    F -->|R√©sultats| G[Gestionnaire de r√©ponses]
    G -->|R√©ponse| A
```

### 4.2 Impl√©mentation du syst√®me de file d'attente

**Fichier :** `/src/osync/proxy/queue/manager.py`

**T√¢ches :**
- [ ] Cr√©er une classe `QueueManager` pour g√©rer la file d'attente
- [ ] Impl√©menter les op√©rations d'ajout, de consultation et de suppression
- [ ] D√©velopper un algorithme de regroupement (batching) pour optimiser les inf√©rences
- [ ] Impl√©menter la priorisation des requ√™tes

**M√©thodes √† impl√©menter :**
```python
class QueueManager:
    def __init__(self):
        self.db = get_db()
        self.sync_manager = SyncManager()
        
    def enqueue(self, request):
        """Ajoute une requ√™te √† la file d'attente."""
        
    def dequeue(self, batch_size=1, model=None):
        """R√©cup√®re des requ√™tes √† traiter."""
        
    def update_request_status(self, request_id, status, result=None):
        """Met √† jour le statut d'une requ√™te."""
        
    def batch_similar_requests(self):
        """Regroupe les requ√™tes similaires pour optimiser le traitement."""
        
    def clean_old_requests(self, max_age_hours=24):
        """Nettoie les requ√™tes anciennes ou trait√©es."""
        
    def get_queue_stats(self):
        """Obtient des statistiques sur la file d'attente actuelle."""
```

### 4.3 Planificateur de t√¢ches

**Fichier :** `/src/osync/proxy/queue/scheduler.py`

**T√¢ches :**
- [ ] D√©velopper un planificateur qui assigne les requ√™tes aux serveurs
- [ ] Impl√©menter des strat√©gies d'√©quilibrage de charge avanc√©es
- [ ] G√©rer les √©checs et les tentatives de r√©essai

**M√©thodes √† impl√©menter :**
```python
class RequestScheduler:
    def __init__(self, queue_manager, cluster_manager):
        self.queue_manager = queue_manager
        self.cluster_manager = cluster_manager
        
    def schedule_next_batch(self):
        """Planifie le traitement du prochain lot de requ√™tes."""
        
    def assign_server(self, request_batch):
        """Assigne un serveur optimal pour traiter un lot de requ√™tes."""
        
    def handle_server_failure(self, server_address):
        """G√®re l'√©chec d'un serveur en r√©assignant ses requ√™tes."""
```

## Phase 5 : Validation et test du syst√®me global

### 5.1 Tests d'int√©gration DB-RAM

**T√¢ches :**
- [ ] Tester la synchronisation entre TinyDB et les structures en m√©moire
- [ ] Valider la coh√©rence des donn√©es apr√®s red√©marrage
- [ ] Mesurer les performances et optimiser les goulots d'√©tranglement

### 5.2 Tests de communication gRPC

**T√¢ches :**
- [ ] V√©rifier que tous les endpoints Ollama sont correctement relay√©s via gRPC
- [ ] Tester la collecte compl√®te des m√©triques des n≈ìuds
- [ ] Valider la transmission fiable des commandes aux n≈ìuds distants

### 5.3 Tests du registre de mod√®les

**T√¢ches :**
- [ ] Tester la distribution des mod√®les √† travers le cluster
- [ ] Valider les op√©rations CRUD sur les mod√®les
- [ ] V√©rifier la coh√©rence du registre apr√®s des pannes de n≈ìuds

### 5.4 Tests du syst√®me de file d'attente

**T√¢ches :**
- [ ] Tester les performances sous charge √©lev√©e
- [ ] Valider le m√©canisme de batching et de priorisation
- [ ] V√©rifier le nettoyage automatique et la gestion des erreurs

## Phase 6 : Optimisations finales et documentation

### 6.1 Optimisations de performance

**T√¢ches :**
- [ ] Optimiser les requ√™tes TinyDB fr√©quentes
- [ ] Am√©liorer les strat√©gies de mise en cache
- [ ] R√©duire la latence du syst√®me de file d'attente

### 6.2 Documentation compl√®te

**T√¢ches :**
- [ ] Documenter l'architecture de persistance et de synchronisation
- [ ] Fournir des guides pour l'utilisation du registre de mod√®les
- [ ] Documenter le syst√®me de file d'attente et ses param√®tres
- [ ] Mettre √† jour les diagrammes d'architecture

## Suivi du progr√®s

| Phase | T√¢che | Statut | Date | Notes |
|-------|-------|--------|------|-------|
| 1.1 | Classe d'abstraction DatabaseManager | ‚úÖ | 2025-04-30 | Impl√©mentation initiale compl√®te |
| 1.2 | Ajout de la d√©pendance √† TinyDB | ‚úÖ | 2025-04-30 | Ajout√© √† pyproject.toml |
| 1.3 | Sch√©ma des donn√©es | ‚úÖ | 2025-04-30 | Toutes les tables d√©finies |
| 2.1 | Architecture de synchronisation | üîÑ | - | Conception termin√©e, impl√©mentation en cours |
| 2.2 | Impl√©mentation de la synchronisation | üîÑ | - | En d√©veloppement |
| 3.1 | Am√©lioration de la communication gRPC | üîÑ | - | Endpoints √† compl√©ter |
| 3.2 | Syst√®me de registre des mod√®les | üî∂ | - | √Ä commencer |
| 4.1 | Architecture de la file d'attente | ‚úÖ | 2025-04-30 | Conception valid√©e |
| 4.2 | Impl√©mentation du syst√®me de file d'attente | üî∂ | - | √Ä commencer |
| 4.3 | Planificateur de t√¢ches | üî∂ | - | √Ä commencer |
| ... | ... | ... | ... | ... |

## Structure r√©vis√©e des d√©pendances entre composants

```mermaid
graph TD
    A[DatabaseManager] --> B[SyncManager]
    B --> C[ClusterManager]
    B --> D[HealthMonitor]
    B --> E[RequestStats]
    B --> F[QueueManager]
    C --> G[ProxyApp]
    D --> G
    E --> G
    F --> H[RequestScheduler]
    H --> G
    I[ModelRegistry] --> C
    I --> J[gRPC Service]
    J --> K[Ollama Nodes]
```

## Simplification de l'approche GRPC

L'approche sync/async de gRPC est simplifi√©e en faveur d'un syst√®me centralis√© de file d'attente au niveau du proxy. Les principales modifications sont :

1. **√âlimination du mode asynchrone complexe** : Le proxy g√®re d√©sormais toutes les files d'attente et le batching
2. **Communication gRPC simplifi√©e** : Les appels aux n≈ìuds sont maintenant synchrones et d√©terministes
3. **Optimisation des ressources** : Le batching est effectu√© c√¥t√© proxy pour une meilleure utilisation des ressources
4. **Pr√©vention des surcharges** : Le planificateur tient compte de la charge des serveurs pour √©viter les surcharges

## Conclusion

Cette nouvelle architecture offre plusieurs avantages significatifs :

1. **Robustesse** : La persistance compl√®te avec TinyDB assure la survie des donn√©es m√™me en cas de red√©marrage
2. **Performance** : La synchronisation DB-RAM offre √† la fois durabilit√© et rapidit√© d'acc√®s
3. **Scalabilit√©** : Le syst√®me de file d'attente permet de g√©rer efficacement les pics de charge
4. **Administration simplifi√©e** : Le registre centralis√© des mod√®les facilite la gestion du cluster
5. **Maintenance facilit√©e** : L'architecture modulaire permet des mises √† jour sans perturbation du service

Les prochaines √©tapes se concentreront sur l'impl√©mentation de ces composants, en commen√ßant par le syst√®me de synchronisation DB-RAM et la communication gRPC am√©lior√©e.